{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Take a look at the files in the directory to better understand the structure of the project. \n",
    "\n",
    "- `task.py`: Define your task (environment) in this file.\n",
    "- `agents/`: Folder containing reinforcement learning agents.\n",
    "    - `policy_search.py`: A sample agent has been provided here.\n",
    "    - `agent.py`: Develop your agent here.\n",
    "- `physics_sim.py`: This file contains the simulator for the quadcopter.  **DO NOT MODIFY THIS FILE**.\n",
    "\n",
    "For this project, you will define your own task in `task.py`.  Although we have provided a example task to get you started, you are encouraged to change it.  Later in this notebook, you will learn more about how to amend this file.\n",
    "\n",
    "You will also design a reinforcement learning agent in `agent.py` to complete your chosen task.  \n",
    "\n",
    "You are welcome to create any additional files to help you to organize your code.  For instance, you may find it useful to define a `model.py` file defining any needed neural network architectures.\n",
    "\n",
    "## Controlling the Quadcopter\n",
    "\n",
    "The agent controls the quadcopter by setting the revolutions per second on each of its four rotors.  The provided agent in the `Basic_Agent` class below always selects a random action for each of the four rotors.  These four speeds are returned by the `act` method as a list of four floating-point numbers.  \n",
    "\n",
    "For this project, the agent that you will implement in `agents/agent.py` will have a far more intelligent method for selecting actions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample task in `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending the task for your purposes, you are welcome to expand the size of the state vector by including the velocity information.  You can use any combination of the pose, velocity, and angular velocity - feel free to tinker here, and construct the state to suit your task.\n",
    "\n",
    "## The Task\n",
    "\n",
    "A sample task has been provided for you in `task.py`.  Open this file in a new window now. \n",
    "\n",
    "The `__init__()` method is used to initialize several variables that are needed to specify the task.  \n",
    "- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n",
    "- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https://arxiv.org/abs/1509.02971).\n",
    "- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n",
    "- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n",
    "- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n",
    "\n",
    "The `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  You can see an example of this in the code cell below.\n",
    "\n",
    "The `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n",
    "\n",
    "In the next section, you will learn how to test the performance of an agent on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The sample agent given in `agents/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n",
    "\n",
    "Run the code cell below to see how the agent performs on the sample task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from agents.policy_search import PolicySearch_Agent\n",
    "from task import Task\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = PolicySearch_Agent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent should perform very poorly on this task.  And that's where you come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "# Make sure to change from notebook to inline after your tests\n",
    "%matplotlib notebook\n",
    "import time\n",
    "\n",
    "class AnimatedPlot():\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize parameters\"\"\"\n",
    "        self.X, self.Y, self.Z = [], [], []\n",
    "\n",
    "        self.fig = plt.figure(figsize = (14,8))\n",
    "        self.ax = self.fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    def plot(self, task, i_episode=None):\n",
    "        pose = task.sim.pose[:3]\n",
    "        self.X.append(pose[0])\n",
    "        self.Y.append(pose[1])\n",
    "        self.Z.append(pose[2])\n",
    "        self.ax.clear()\n",
    "        if i_episode:\n",
    "            plt.title(\"Episode {}\".format(i_episode))\n",
    "\n",
    "        if len(self.X)>1:\n",
    "            self.ax.scatter(self.X[:-1], self.Y[:-1], self.Z[:-1], c='k', alpha=0.3)\n",
    "        if task.sim.done and task.sim.runtime > task.sim.time:\n",
    "            # Colision\n",
    "            self.ax.scatter(pose[0], pose[1], pose[2], c='r', marker='*', linewidths=5)\n",
    "        else:\n",
    "            self.ax.scatter(pose[0], pose[1], pose[2], c='k', marker='s', linewidths=5)\n",
    "\n",
    "        self.fig.canvas.draw()\n",
    "        time.sleep(0.5)\n",
    "        #plt.pause(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 436 / Reward: -0.13 / Cumulative Reward: 59.30 / Best Reward: 110.523\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-05f36c1271d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/rl-quadcopter/agents/agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Roll over last state and action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/rl-quadcopter/agents/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Compute Q targets for current states and train critic model (local)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Train actor model (local)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## TODO: Train your agent here.\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from agents.agent import DDPG\n",
    "from agents.actor import Actor\n",
    "from agents.critic import Critic\n",
    "from agents.aux import OUNoise\n",
    "from task import Task\n",
    "\n",
    "\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 50., 0., 0., 0.])   # initial pose\n",
    "target_pos = np.array([0., 0., 5.])        # target pose\n",
    "#init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "#init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "file_output = 'sim.txt'           # file name for saved results\n",
    "\n",
    "\n",
    "# Setup\n",
    "#task = Task(init_pose, init_velocities, init_angle_velocities, runtime, target_pos)\n",
    "task = Task(init_pose,target_pos)\n",
    "num_episodes = 2000\n",
    "\n",
    "\n",
    "# Initialize DDPG Agent \n",
    "\n",
    "actor = Actor(task.state_size,task.action_size,task.action_low,task.action_high)\n",
    "critic = Critic(task.state_size,task.action_size)\n",
    "\n",
    "agent = DDPG(task)\n",
    "\n",
    "samp_rewards = []\n",
    "\n",
    "best_reward = -100000\n",
    "\n",
    "\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4', 'reward', 'distance']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = agent.reset_episode() # start a new episode\n",
    "\n",
    "        samp_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.act(state) \n",
    "            next_state, reward, done = task.step(action)\n",
    "            agent.step(action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            x=abs(task.sim.pose[:3] - target_pos).sum()\n",
    "            #samp_reward += reward\n",
    "            \n",
    "            #if i_episode in (1,2,10,50,100):\n",
    "            #    print(\"Episode: {0} / Time: {3:.2f} / Distance: {1:.2f} / Step Reward: {5:.2f} / C.Reward: {2:.2f} / Pose: {4}\".format(i_episode,x,samp_reward,step, task.sim.pose[:3],reward))#,end=\"\\r\")\n",
    "                      \n",
    "            \n",
    "            if done:                \n",
    "                if reward > best_reward:\n",
    "                    best_reward = reward  \n",
    "                                        \n",
    "                to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(action) + [reward] + [x]\n",
    "\n",
    "                for ii in range(len(labels)):\n",
    "                    results[labels[ii]].append(to_write[ii])\n",
    "\n",
    "                writer.writerow(to_write)            \n",
    "                #print(\"X: {4:.2f} Episode: {0} / Reward: {1:.2f} / Cumulative Reward: {2:.2f} / Best Reward: {3:.2f}\".format(i_episode,reward,samp_reward,best_reward,x),end=\"\\r\")  # [debug]               \n",
    "                print(\"Episode: {0} / Best Reward: {3:.2f} / Reward: {1:.2f} // Distance: {4:.2f} // Pose: {5}\".format(i_episode,reward,samp_reward,best_reward,x,task.sim.pose[:3]),end=\"\\r\")  # [debug]               \n",
    "                \n",
    "                \n",
    "                break\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AN INSTANCE OF THE CLASS BEFORE THE WHILE LOOP\n",
    "myplot = AnimatedPlot()\n",
    "state = agent.reset_episode() # start a new episode\n",
    "while True:\n",
    "    action = agent.act(state) \n",
    "    next_state, reward, done = task.step(action)\n",
    "    agent.step(action, reward, next_state, done)\n",
    "    state = next_state\n",
    "\n",
    "    # CALL THE METHOD plot(task)\n",
    "    myplot.plot(task)\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['reward'], label='Reward')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect Reward Function \n",
    "\n",
    "#!conda update seaborn -y\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "\n",
    "x = np.linspace(0,300,300)\n",
    "y = -30*np.tan(0.01*x+1.65)-300\n",
    "#y = -1*np.tanh(0.01*x-2)\n",
    "\n",
    "data = pd.read_csv('sim.txt')\n",
    "\n",
    "# sns.lmplot(x=\"distance\", y=\"reward\", data=data, ax=ax2, color='r')\n",
    "# sns.lineplot(y=y,x=x,ax=ax)\n",
    "# plt.xlim(0,25)\n",
    "# plt.ylim(-750,250)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# sns.regplot(x=x, y=y, ax=ax, fit_reg= 0)\n",
    "# ax2 = ax.twinx()\n",
    "sns.regplot(x=\"distance\", y=\"reward\", data=data)#, ax=ax2, color='r',fit_reg=0)\n",
    "plt.xlim(0,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(y='reward',x='time', hue='distance', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Train your agent here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**: The Agent's task was to take off and go to a predifined point. The reward function was choosen after a lot of tries. After tried with the function provided, I used a very simple reward function: $y = 1 - x^0.4$. Althought it did the job, it wasn't good enough. Then I tried the Euclidean Distance, which give me a good result. Inquietly, I keeped lookingfor a better one. So I tailored one myself, which has the best result: $ y = -30*tan(0.01*x+1.65)-300$. This function has gives positives rewards if the agent is closer than 2 points, maximazing its score, and negative ones to farter distances. The tangent function makes the transition easy and predictable for the agent. And it gives a big reward when the agent reaches the position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-33d8228a7791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1.65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#y = -1*np.tanh(0.01*x-2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pylab\n",
    "x = np.linspace(0,300,300)\n",
    "y = -150*np.tan(0.01*x+1.65)-500\n",
    "#y = -1*np.tanh(0.01*x-2)\n",
    "\n",
    "# compose plot\n",
    "pylab.plot(y)\n",
    "pylab.ylim(-500,350)\n",
    "pylab.xlim(-1,300)\n",
    "pylab.show() # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
